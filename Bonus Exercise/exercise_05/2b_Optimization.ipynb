{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update rules\n",
    "So far we have used vanilla stochastic gradient descent (SGD) as our update rule. More sophisticated update rules can make it easier to train deep networks. We will implement a few of the most commonly used update rules and compare them to vanilla SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_code.networks.optimizer import sgd_momentum\n",
    "import numpy as np\n",
    "import os\n",
    "from exercise_code.data.csv_dataset import CSVDataset\n",
    "from exercise_code.data.csv_dataset import FeatureSelectorAndNormalizationTransform\n",
    "from exercise_code.data.dataloader import DataLoader\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the data as we did in the last task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "target_column = 'SalePrice'\n",
    "i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "root_path = os.path.join(i2dl_exercises_path, \"datasets\", 'housing')\n",
    "housing_file_path = os.path.join(root_path, \"housing_train.csv\")\n",
    "download_url = 'https://cdn3.vision.in.tum.de/~dl4cv/housing_train.zip'\n",
    "\n",
    "# Always make sure this line was run at least once before trying to\n",
    "# access the data manually, as the data is downloaded in the \n",
    "# constructor of CSVDataset.\n",
    "train_dataset = CSVDataset(target_column=target_column, root=root_path, download_url=download_url, mode=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df = train_dataset.df\n",
    "target_column = 'SalePrice'\n",
    "# Select only 2 features to keep plus the target column.\n",
    "selected_columns = ['GrLivArea', target_column]\n",
    "mn, mx, mean = df.min(), df.max(), df.mean()\n",
    "\n",
    "column_stats = {}\n",
    "for column in selected_columns:\n",
    "    crt_col_stats = {'min' : mn[column],\n",
    "                     'max' : mx[column],\n",
    "                     'mean': mean[column]}\n",
    "    column_stats[column] = crt_col_stats    \n",
    "\n",
    "transform = FeatureSelectorAndNormalizationTransform(column_stats, target_column)\n",
    "\n",
    "def rescale(data, key = \"SalePrice\", column_stats = column_stats):\n",
    "    \"\"\" Rescales input series y\"\"\"\n",
    "    mx = column_stats[key][\"max\"]\n",
    "    mn = column_stats[key][\"min\"]\n",
    "\n",
    "    return data * (mx - mn) + mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = CSVDataset(mode=\"train\", target_column=target_column, root=root_path, download_url=download_url, transform=transform)\n",
    "val_dataset = CSVDataset(mode=\"val\", target_column=target_column, root=root_path, download_url=download_url, transform=transform)\n",
    "test_dataset = CSVDataset(mode=\"test\", target_column=target_column, root=root_path, download_url=download_url, transform=transform)\n",
    "\n",
    "print(\"Number of training samples:\", len(train_dataset))\n",
    "print(\"Number of validation samples:\", len(val_dataset))\n",
    "print(\"Number of test samples:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from exercise_code.networks.optimizer import SGD, sgd_momentum, Adam\n",
    "from exercise_code.networks.loss import MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from exercise_code.networks.regression_net import RegressionNet\n",
    "from exercise_code.solver import Solver\n",
    "\n",
    "batch_size = 2\n",
    "lr = 1e-3\n",
    "hidden_size = 1\n",
    "std = 1.\n",
    "epochs = 20\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent\n",
    "You have implemented the SGD in previous tasks, now lets look at it again.\n",
    "In stochastic gradient descent, the true gradient of $L(\\theta)$ is approximated by a gradient at a single example/mini-batch:\n",
    "$${\\displaystyle \\theta:=\\theta-\\eta \\nabla L(\\theta)=\\theta-{\\frac {\\eta }{n}}\\sum _{i=1}^{n}\\nabla L_{i}(\\theta),}$$\n",
    "Here is how the training and validation curve looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "solvers = {}\n",
    "\n",
    "print('running with SGD ')\n",
    "model = RegressionNet(input_size=1, hidden_size=hidden_size, std=std)\n",
    "\n",
    "solver = Solver(model, train_dataloader, val_dataloader, learning_rate=lr,\n",
    "                    loss_func=MSE(), optimizer=SGD)\n",
    "\n",
    "solvers[0] = solver\n",
    "solver.train(20)\n",
    "\n",
    "\n",
    "ax1 = plt.subplot(2, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "ax2 = plt.subplot(2, 1, 2)\n",
    "plt.title('Validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "ax1 = plt.subplot(2, 1, 1)\n",
    "ax1.plot(solver.train_loss_history, '-o', label='SGD')\n",
    "\n",
    "ax2 = plt.subplot(2, 1, 2)\n",
    "ax2.plot(solver.val_loss_history, '-o', label='SGD')\n",
    "\n",
    "for i in [1, 2]:\n",
    "    ax = plt.subplot(2, 1, i)\n",
    "    ax.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD+Momentum\n",
    "Stochastic gradient descent with momentum is a widely used update rule that tends to make deep networks converge faster than vanilla stochstic gradient descent.\n",
    "$$v^{k+1} = \\beta v^{k} - \\alpha \\nabla_{\\theta} L (\\theta^{k}) $$\n",
    "\n",
    "$$\\theta^{k+1} = \\theta^{k} + v^{k+1}$$\n",
    "\n",
    "Open the file `exercise_code/networks/optimizer.py` and read the documentation at the top of the file to make sure you understand the API. We have implemented the SGD+momentum update rule in the class `sgd_momentum` for you.\n",
    "Check it out to make sure you understand it fully.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see how the training and validation curve looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "solvers = {}\n",
    "\n",
    "for update_rule in ['sgd', 'sgd_momentum']:\n",
    "    \n",
    "    print('running with ', update_rule)\n",
    "    model = RegressionNet(input_size=1, hidden_size=hidden_size, std=std)\n",
    "    \n",
    "    if update_rule == 'sgd_momentum':\n",
    "        solver = Solver(model, train_dataloader, val_dataloader, learning_rate=lr,\n",
    "                        loss_func=MSE(), optimizer=sgd_momentum)\n",
    "    \n",
    "    elif update_rule == 'sgd':\n",
    "        solver = Solver(model, train_dataloader, val_dataloader, learning_rate=lr, \n",
    "                        loss_func=MSE(), optimizer=SGD)\n",
    "    \n",
    "    solvers[update_rule] = solver\n",
    "    solver.train(20)\n",
    "    \n",
    "\n",
    "ax1 = plt.subplot(2, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "ax2 = plt.subplot(2, 1, 2)\n",
    "plt.title('Validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "\n",
    "for update_rule, solver in solvers.items():\n",
    "    ax1 = plt.subplot(2, 1, 1)\n",
    "    ax1.plot(solver.train_loss_history, '-o', label=update_rule)\n",
    "  \n",
    "    ax2 = plt.subplot(2, 1, 2)\n",
    "    ax2.plot(solver.val_loss_history, '-o', label=update_rule)\n",
    "\n",
    "for i in [1, 2]:\n",
    "    ax = plt.subplot(2, 1, i)\n",
    "    ax.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam\n",
    "Adam [1] is an update rule that sets per-parameter learning rates by using a running average of the second moments of gradients.\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\\nu_{t}=\\beta_{1} * \\nu_{t-1}-\\left(1-\\beta_{1}\\right) * g_{t} \\\\ s_{t}=\\beta_{2} * s_{t-1}-\\left(1-\\beta_{2}\\right) * g_{t}^{2} \\\\ \\Delta \\theta_{t}=-\\eta \\frac{\\nu_{t}}{\\sqrt{s_{t}+\\epsilon}} * g_{t} \\\\ \\theta_{t+1}=\\theta_{t}+\\Delta \\theta_{t}\\end{array}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\\eta: \\text { Initial Learning rate} \\\\ g_{t}: \\text { Gradient at time t along } \\theta^{j} \\\\ \\nu_{t}: \\text { Exponential Average of gradients along } \\theta_{j} \\\\ s_{t}: \\text { Exponential Average of squares of gradients along } \\theta_{j} \\\\ \\beta_{1}, \\beta_{2}: \\text { Hyperparameters}\\end{array}\n",
    "$$\n",
    "\n",
    "In the file `exercise_code/networks/optimizer.py`, we have implemented the update rule `adam` for you. Check this implementation and make sure you understand what the optimizer is doing. Then train the fully connected net below to see how Adam affects the learning process.\n",
    "\n",
    "[1] Diederik Kingma and Jimmy Ba, \"Adam: A Method for Stochastic Optimization\", ICLR 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#learning_rates = {'adam': 8e-4}\n",
    "update_rule = 'adam'\n",
    "print('running with ', update_rule)\n",
    "model = RegressionNet(input_size=1, hidden_size=hidden_size, std=std)\n",
    "solver = Solver(model, train_dataloader, val_dataloader, learning_rate=lr, loss_func=MSE(), optimizer=Adam)\n",
    "\n",
    "solvers[update_rule] = solver\n",
    "solver.train(20)\n",
    "print()\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "\n",
    "\n",
    "for update_rule, solver in solvers.items():\n",
    "    ax1 = plt.subplot(2, 1, 1)\n",
    "    plt.plot(solver.train_loss_history, '-o', label=update_rule)\n",
    "  \n",
    "    ax2 =plt.subplot(2, 1, 2)\n",
    "    plt.plot(solver.val_loss_history, '-o', label=update_rule)\n",
    "\n",
    "for i in [1, 2]:\n",
    "    plt.subplot(2, 1, i)\n",
    "    plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "i2dl",
   "language": "python",
   "name": "i2dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
