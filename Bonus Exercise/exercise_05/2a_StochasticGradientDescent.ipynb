{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent & Stochastic Gradient Descent (SDG)\n",
    "In this notebook, we will discuss the effect of batch training.\n",
    "You will see the  difference between full dataset and batch training.\n",
    "\n",
    "Before we start, let's load the data and do the pre-processing again like in previous tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from exercise_code.data.csv_dataset import CSVDataset\n",
    "from exercise_code.data.csv_dataset import FeatureSelectorAndNormalizationTransform\n",
    "from exercise_code.data.dataloader import DataLoader\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from exercise_code.networks.optimizer import SGD, sgd_momentum, Adam\n",
    "from exercise_code.networks.loss import MSE\n",
    "from exercise_code.networks.regression_net import RegressionNet\n",
    "from exercise_code.solver import Solver\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "target_column = 'SalePrice'\n",
    "i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "root_path = os.path.join(i2dl_exercises_path, \"datasets\", 'housing')\n",
    "housing_file_path = os.path.join(root_path, \"housing_train.csv\")\n",
    "download_url = 'https://cdn3.vision.in.tum.de/~dl4cv/housing_train.zip'\n",
    "\n",
    "# Always make sure this line was run at least once before trying to\n",
    "# access the data manually, as the data is downloaded in the\n",
    "# constructor of CSVDataset.\n",
    "train_dataset = CSVDataset(target_column=target_column, root=root_path, download_url=download_url, mode=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = train_dataset.df\n",
    "\n",
    "# Select only 2 features to keep plus the target column.\n",
    "selected_columns = ['GrLivArea','GarageArea', target_column]\n",
    "mn, mx, mean = df.min(), df.max(), df.mean()\n",
    "\n",
    "column_stats = {}\n",
    "for column in selected_columns:\n",
    "    crt_col_stats = {'min' : mn[column],\n",
    "                     'max' : mx[column],\n",
    "                     'mean': mean[column]}\n",
    "    column_stats[column] = crt_col_stats\n",
    "\n",
    "transform = FeatureSelectorAndNormalizationTransform(column_stats, target_column)\n",
    "\n",
    "def rescale(data, key = \"SalePrice\", column_stats = column_stats):\n",
    "    \"\"\" Rescales input series y\"\"\"\n",
    "    mx = column_stats[key][\"max\"]\n",
    "    mn = column_stats[key][\"min\"]\n",
    "\n",
    "    return data * (mx - mn) + mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = CSVDataset(mode=\"train\", target_column=target_column, root=root_path, download_url=download_url, transform=transform)\n",
    "val_dataset = CSVDataset(mode=\"val\", target_column=target_column, root=root_path, download_url=download_url, transform=transform)\n",
    "test_dataset = CSVDataset(mode=\"test\", target_column=target_column, root=root_path, download_url=download_url, transform=transform)\n",
    "\n",
    "print(\"Number of training samples:\", len(train_dataset))\n",
    "print(\"Number of validation samples:\", len(val_dataset))\n",
    "print(\"Number of test samples:\", len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "Here you can set your hyperparameters that are used across the notebook. You can play around how they have an effect on the training of your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "hidden_size = 4\n",
    "std = .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory\n",
    "First, let's see how much memory is needed during one iteration. In a forward pass, certain value of the layers should be stored into cache for later gradient calculation. And in a backward pass, the gradient has to be stored for parameter update. Here we discuss the memory these values occupy in GD and SGD. \n",
    "\n",
    "We can simply use `sys.getsizeof()` function to return the size of an object in bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 1\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "# get one batch from train_dataloader\n",
    "data = next(iter(train_dataloader))\n",
    "X = data['features']\n",
    "y = data['target']\n",
    "model = RegressionNet(input_size=len(selected_columns)-1, hidden_size=hidden_size, std=std)\n",
    "solver = Solver(model, train_dataloader, val_dataloader, learning_rate=lr,\n",
    "                        loss_func=MSE(), optimizer=SGD)\n",
    "solver._step(X, y, validation=False)\n",
    "\n",
    "print('Memory required in bytes:', model.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now if we do an iteration on the full dataset, let's see how much memory is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# use full batch\n",
    "batch_size = len(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "data = next(iter(train_dataloader))\n",
    "X = data['features']\n",
    "y = data['target']\n",
    "model = RegressionNet(input_size=len(selected_columns)-1, hidden_size=hidden_size, std=std)\n",
    "solver = Solver(model, train_dataloader, val_dataloader, learning_rate=lr,\n",
    "                        loss_func=MSE(), optimizer=SGD)\n",
    "solver._step(X, y, validation=False)\n",
    "print('Memory required in bytes:', model.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there is a huge difference in required memory during one iteration.\n",
    "As the model capacity and the size of dataset increase, training on full batch will become impossible due to the high memory requirement.\n",
    "You can try to change the hidden_size and batch_size to see how the required memory changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Computation\n",
    "The number of operation is also different for GD and SGD. Here we first discuss how to calculate the number of operation needed in forwad and backward pass.\n",
    "### Forward Pass\n",
    "In an affine layer, it is  simply a matrix multiplication. To go from layer ùëñ to ùëó, you do $ùëÜ_{mn}=ùëä_{md}‚àóùëç_{dn}$. This operation has time complexity $O(m*d*n)$.\n",
    "\n",
    "Then we apply activation function $ùëç_{mn}=f(ùëÜ_{mn})$. Since it is a element-wise operation, it has time complexity $O(m*n)$\n",
    "\n",
    "### Backward Pass\n",
    "The same as in forward pass. In affine layer, it calculates two gradients. One is wrt. weight $O(m*d*n)$ and onther one is wrt. the input $O(m*d*n)$.\n",
    "\n",
    "In activation function, we have time complexity $O(m*n)$.\n",
    "\n",
    "Here, you can see the different number of operation when using mini-batch and full batch in one iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for batch_size in [32, len(train_dataset)]:\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    # get one batch from train_dataloader\n",
    "    data = next(iter(train_dataloader))\n",
    "    X = data['features']\n",
    "    y = data['target']\n",
    "    model = RegressionNet(input_size=len(selected_columns)-1, hidden_size=hidden_size, std=std)\n",
    "    solver = Solver(model, train_dataloader, val_dataloader, learning_rate=lr,\n",
    "                            loss_func=MSE(), optimizer=SGD)\n",
    "    solver._step(X, y, validation=False)\n",
    "    print('number of operation:', model.num_operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training \n",
    "In this section, you will see how the loss per iteration changes when using SDG and GD. As SGD computes gradient on a small batch of data, the gradient will flactuate because it captures the noise in the batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "solvers = {}\n",
    "num_computation = {}\n",
    "train_mode = [*[('mini-batch {}'.format(i), i) for i in [1,  32, 64]], ('full batch', len(train_dataset))]\n",
    "\n",
    "for i in train_mode:\n",
    "    batch_size = i[1]\n",
    "    print('running with batch_size = ', batch_size )\n",
    "    model = RegressionNet(input_size=len(selected_columns)-1, hidden_size=hidden_size, std=std)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    solver = Solver(model, train_dataloader, val_dataloader, learning_rate=lr,\n",
    "                        loss_func=MSE(), optimizer=SGD)\n",
    "\n",
    "    solvers[i[0]] = solver\n",
    "    solver.train(epochs=10)\n",
    "    num_computation[i[0]] = solver.num_operation\n",
    "ax1 = plt.subplot(2, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "ax2 = plt.subplot(2, 1, 2)\n",
    "plt.title('Validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "\n",
    "\n",
    "for train_mode, solver in solvers.items():\n",
    "\n",
    "    x1 = np.linspace(0, 9, len(solver.train_batch_loss))\n",
    "    x2 = np.linspace(0, 9, len(solver.val_batch_loss))\n",
    "\n",
    "    ax1 = plt.subplot(2, 1, 1)\n",
    "    ax1.plot(x1, solver.train_batch_loss, '-o', label=train_mode)\n",
    "\n",
    "    ax2 = plt.subplot(2, 1, 2)\n",
    "    ax2.plot(x2, solver.val_batch_loss, '-o', label=train_mode)\n",
    "\n",
    "for i in [1, 2]:\n",
    "    ax = plt.subplot(2, 1, i)\n",
    "    ax.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Which one is better for generalization? \n",
    "When you look at the validation loss, the SGD loss jumps up and down due to the noisy data.\n",
    "In GD it does not have this issue, because it looks at the whole dataset. So it can learn the representation of the whole dataset, therefore has it better generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Converge speed\n",
    "\n",
    "Now, let's see how the loss changes against the number of computation.\n",
    "Training with smaller batch size will converge faster compared to full data. Why?\n",
    "\n",
    "While in GD, you have to run through all the samples in your training set to do a single update for a parameter in a particular iteration. In SGD, on the other hand, you use only one of subset of training sample from your training set to do the update for a parameter in a particular iteration.\n",
    "Using SGD will be faster because you use only subset of samples and it starts improving itself right away from that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('number of computation')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Validation loss')\n",
    "plt.xlabel('number of computation')\n",
    "\n",
    "\n",
    "\n",
    "for update_rule, solver in solvers.items():\n",
    "    x = np.linspace(0, solver.num_operation, 10)\n",
    "\n",
    "    ax1 = plt.subplot(2, 1, 1)\n",
    "    plt.plot(x, solver.train_loss_history, '-o', label=update_rule)\n",
    "\n",
    "    ax2 =plt.subplot(2, 1, 2)\n",
    "    plt.plot(x, solver.val_loss_history, '-o', label=update_rule)\n",
    "\n",
    "\n",
    "for i in [1, 2]:\n",
    "    plt.subplot(2, 1, i)\n",
    "    plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "i2dl",
   "language": "python",
   "name": "i2dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
