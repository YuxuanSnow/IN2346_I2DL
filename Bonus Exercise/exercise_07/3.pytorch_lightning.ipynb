{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start again by installing pytorch lightning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lightning version: 0.7.6\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-lightning==0.7.6 > /dev/null\n",
    "import pytorch_lightning as pl\n",
    "print(f\"Lightning version: {pl.__version__}\")\n",
    "if not pl.__version__.startswith(\"0.7.6\"):\n",
    "    print(\"You are using another version of pytorch lightning. We expect pytorch lightning 0.7.6. You can continue with your version but it\"\n",
    "          \" might cause some issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "Writing code for deep learning models is not always exciting. Between different models there exist large overlaps, such as the solver, that need to be adapted for each project. Additionally, organizational methods such as logging hyperparameters that have been used for an experiment or logging the training and validation loss don't change from one project to the next. However, in the beginning it may be hard and time-consuming to find the best method for that. In this notebook, we propose a method that provides a nice solution for these problems, the framework PyTorch Lightning, based on PyTorch.\n",
    "\n",
    "\n",
    "### Idea behind PyTorch Lightning\n",
    "Following our observations, we can classify deep learning code into three categories\n",
    "* Research code (the exciting part!, changes with new tasks, models etc.) $\\rightarrow$ into new LightningModule\n",
    "* Engineering code (the same for all projects and models) $\\rightarrow$ Trainer class\n",
    "* Non-essential code (logging, organizing runs – very important, but should be automated) $\\rightarrow$ Callbacks \n",
    "\n",
    "PyTorch Lightning introduces three solutions for these categories, the LightningModule, a Trainer and Callbacks. \n",
    "\n",
    "1. **LightningModules** contain all model related code. This is the part where we are working on when creating a new project. The idea is to have all important code in one module, e.g., the model's architecture and the evalutation of training and validation metrics. This provides a better overview as repeated elements, such as the training procedure, are not stored in the code that we work on. The lightning module also handles the calls `.to(device)` or `.train()` and `.eval()`. Hence, there is no need anymore to switch between the cpu and gpu and to take care of the model's mode as this is automated by the LightningModule. The framework also enables easy parallel computation on multiple gpus. \n",
    "\n",
    "2. **Trainer** contains all code needed for training our neural networks that doesn't change for each project (\"one size fits all\"). Usually, we don't touch the code automated by this class. The arguments that are specific for one training such as learning rate and batch size are provided as initialization arguments for the LightningModule.\n",
    "\n",
    "3. **Callbacks** automate all parts needed for logging hyperparameters or training results such as the tensorboard logger. Logging becomes very important for research later since the results of experiments need to be reproducible.\n",
    "\n",
    "All in all, PyTorch is a framework that handles all (annoying) \"engeneering\" stuff for you such that you have more time for exciting research and scientific coding. This also results in the advantage that automated parts are guaranteed to be bug-free. Hence, you can't include a bug in a part of your code that is often used but not often checked. PyTorch Lightning is very young (it only started last summer) with an active developping community. There are lots of features to come and if you're missing a feature, often, the solution will be added within some days after opening an issue on github."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Research relevant code goes into the LightningModule:\n",
    "advantage: also training & validation steps are in this module as they change from task to task or model to model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://github.com/PyTorchLightning/pytorch-lightning/raw/master/docs/source/_images/lightning_module/pt_to_pl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining code is automated in the Trainer class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://github.com/PyTorchLightning/pytorch-lightning/raw/master/docs/source/_images/lightning_module/pt_trainer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Startet With PyTorch Lightning\n",
    "\n",
    "### 1. Define A LightningModule\n",
    "\n",
    "We define our network as an instance of `pl.LightningModule` which replaces your old network based on the class `nn.Module`. Additionally, it contains all relevant parts that are used for training and evaluating different models on various tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward pass and `__init__()` remain the same, we can just copy the code from the `nn.Module`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class TwoLayerNet(pl.LightningModule):\n",
    "    def __init__(self, hparams, input_size=1 * 28 * 28, hidden_size=512, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.hparams = hparams\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hidden_size, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten the image first\n",
    "        N, _, _, _ = x.shape\n",
    "        x = x.view(N, -1)\n",
    "\n",
    "        x = self.model(x)\n",
    "\n",
    "        return x \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training steps and validation steps also change with different tasks, losses, or evaluation metrics. Consequently, it is useful to integrate these parts in the model. Validation loss is logged for each validation iteration and aggregated at the end of the epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, targets = batch\n",
    "\n",
    "        # forward pass\n",
    "        out = self.forward(images)\n",
    "\n",
    "        # loss\n",
    "        loss = F.cross_entropy(out, targets)\n",
    "\n",
    "        # accuracy\n",
    "        _, preds = torch.max(out, 1)  # convert output probabilities to predicted class\n",
    "        acc = preds.eq(targets).sum() / targets.size(0)\n",
    "\n",
    "        # logs\n",
    "        tensorboard_logs = {'loss': loss, 'acc': acc}\n",
    "\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, targets = batch\n",
    "\n",
    "        # forward pass\n",
    "        out = self.forward(images)\n",
    "\n",
    "        # loss\n",
    "        loss = F.cross_entropy(out, targets)\n",
    "\n",
    "        # accuracy\n",
    "        _, preds = torch.max(out, 1)\n",
    "        acc = preds.eq(targets).sum() / targets.size(0)\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            self.visualize_predictions(images, out.detach(), targets)\n",
    "\n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x['val_acc'] for x in outputs]).mean()\n",
    "\n",
    "        tensorboard_logs = {'val_loss': avg_loss, 'val_acc': avg_acc}\n",
    "\n",
    "        return {'val_loss': avg_loss, 'log': tensorboard_logs}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also integrate the data loader into our model. For that, we first define our datasets in the method `prepare_data()` that is called after the initialization of the LightningModule. Afterward, we can call the datasets in the methods `train_dataloader()`and `val_dataloader()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    def prepare_data(self):\n",
    "        # download\n",
    "        transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "        fashion_mnist_train = torchvision.datasets.FashionMNIST(root='../datasets', train=True,\n",
    "                                                                  download=True, transform=transform)\n",
    "\n",
    "        fashion_mnist_test = torchvision.datasets.FashionMNIST(root='../datasets', train=False,\n",
    "                                                                  download=True, transform=transform)\n",
    "\n",
    "        # train/val split\n",
    "        torch.manual_seed(0)\n",
    "        train_dataset, val_dataset = random_split(fashion_mnist_train, [50000, 10000])\n",
    "        torch.manual_seed(torch.initial_seed())\n",
    "\n",
    "        # assign to use in dataloaders\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.test_dataset = fashion_mnist_test\n",
    "\n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, shuffle=True, batch_size=self.hparams[\"batch_size\"])\n",
    "\n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.hparams[\"batch_size\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step that is missing in our LightningModule is the optimizer. This method needs to be defined in every LightningModule (together with init, forward, training_step, training_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(self.model.parameters(), self.hparams[\"learning_rate\"], momentum=0.9)\n",
    "\n",
    "        return optim\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the code samples of all these steps existed before. Lightning just rearranges them and stores the important parts in a new module. The remaining code is automated in the Training class:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2. Fit the model with a Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the model with a set of hyperparameters given in the dictionary `hparams`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "**Remark:** Set the option `weights_summary=None` to avoid that a summary of the weights is printed at the start of each training\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "No environment variable for node rank defined. Set as 0.\n",
      "/home/yuxuan/anaconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:23: RuntimeWarning: You have defined a `val_dataloader()` and have defined a `validation_step()`, you may also want to define `validation_epoch_end()` for accumulating stats.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name    | Type       | Params\n",
      "-----------------------------------\n",
      "0 | model   | Sequential | 407 K \n",
      "1 | model.0 | Linear     | 401 K \n",
      "2 | model.1 | Sigmoid    | 0     \n",
      "3 | model.2 | Linear     | 5 K   \n",
      "/home/yuxuan/anaconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:23: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxuan/anaconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:23: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "919fc38366fc4cfa8ecb3f72133f41d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from exercise_code.lightning_models import TwoLayerNet\n",
    "\n",
    "hparams = {\n",
    "    \"batch_size\": 16,\n",
    "    \"learning_rate\": 1e-3,\n",
    "}\n",
    "\n",
    "model = TwoLayerNet(hparams)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    # weights_summary=None,\n",
    "    max_epochs=2,\n",
    ")\n",
    "\n",
    "trainer.fit(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkout the directory `./lightning_logs`. For each run there is a new directory version_xx. The most right argument in the tqdm bar above shows the version of the current run. Each directory automatically contains a folder with checkpoints, tensorboard logs and the hyperparameters for this run.\n",
    "\n",
    "As seen in the last notebook, you can have a look at the tensorboard logs of the runs with the command (in your terminal, opened at folder exercise_07)\n",
    "```\n",
    "tensorboard --logdir lightning_logs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add images to tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensorboard logger is a submodule of the LightningModule and can be accessed via `self.logger`. We can simply add images or anything else to the logging module by calling for example\n",
    "```python\n",
    "self.logger.experiment.add_image('tag', image)\n",
    "```\n",
    "to add an image. In the following we log the first batch of validation images in a grid together with the predicted class labels and the ground truth labels (in the second row). We add the lines\n",
    "```python\n",
    "        if batch_idx == 0:\n",
    "            self.visualize_predictions(images, out.detach(), targets)\n",
    "```\n",
    "to the validation step that calls the following function:\n",
    "```python\n",
    "    def visualize_predictions(self, images, preds, targets):\n",
    "        class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "                       'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "        # determine size of the grid based on given batch size\n",
    "        num_rows = torch.tensor(len(images)).float().sqrt().floor()\n",
    "        \n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "        for i in range(len(images)):\n",
    "            plt.subplot(num_rows ,len(images) // num_rows + 1, i+1)\n",
    "            plt.imshow(images[i].permute(1, 2, 0))\n",
    "            plt.title(class_names[torch.argmax(preds, axis=-1)[i]] + f'\\n[{class_names[targets[i]]}]')\n",
    "            plt.axis('off')\n",
    "\n",
    "        self.logger.experiment.add_figure('predictions', fig, global_step=self.global_step)\n",
    "```\n",
    "You can view the logged images in your tensorboard under the tab \"Images\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Features in PyTorch Lightning\n",
    "\n",
    "The Trainer class contains some nice arguments, that make debugging in deep learning easier.\n",
    "\n",
    "### Check your timings\n",
    "The argument `profiler=True` measures the timings of dataloading, forward and backward pass for you. Run the cell below. After 3 epochs (`max_epochs=3`) the training stops and an overview of the timing is printed. This feature enables you to detect bottlenecks in your model. A bottleneck can be for example long times in dataloading. It becomes very important later, when you start to implement custom layers or loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "No environment variable for node rank defined. Set as 0.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e221e90e794bd68f4e056f152edf31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Profiler Report\n",
      "\n",
      "Action              \t|  Mean duration (s)\t|  Total time (s) \n",
      "-----------------------------------------------------------------\n",
      "on_train_start      \t|  0.037534       \t|  0.037534       \n",
      "on_epoch_start      \t|  0.003          \t|  0.0059999      \n",
      "get_train_batch     \t|  0.0035113      \t|  21.953         \n",
      "on_batch_start      \t|  2.2796e-05     \t|  0.14247        \n",
      "model_forward       \t|  0.00081371     \t|  5.0857         \n",
      "model_backward      \t|  0.0014863      \t|  9.2896         \n",
      "on_after_backward   \t|  3.589e-06      \t|  0.022431       \n",
      "optimizer_step      \t|  0.00084983     \t|  5.3114         \n",
      "on_batch_end        \t|  0.0018373      \t|  11.483         \n",
      "on_epoch_end        \t|  1.4519e-05     \t|  2.9038e-05     \n",
      "on_train_end        \t|  0.0012562      \t|  0.0012562      \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    weights_summary=None,\n",
    "    profiler=True,\n",
    "    max_epochs=2,\n",
    ")\n",
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging Options\n",
    "\n",
    "There are many other debugging options provided with the Trainer class. A few useful one are\n",
    "* `fast_dev_run=True`: Runs of batch of each train, val and test pass (if val and test are implemented). A fast way to check if everything works (dataloading, validation metric, model saving/ loading) without having to wait for a full epoch.\n",
    "* `track_grad_norm=2`: Logs the l2-norm of the gradients (set to 1 for the l1-norm) for each layer in tensorboard. You can check wether the network is actually doing something. If the gradients are too small or too high, you won't have a good training (cf. vanishing/ exploding gradients)\n",
    "\n",
    "\n",
    "### Other Features\n",
    "\n",
    "Finally, we want to mention some other useful options in the Trainer class:\n",
    "* `max_epochs`: Set the maximal number of epochs\n",
    "* `weights_summary`: Prints a summary of the number of weights per layer at the beginning of the training. Set to None to avoid that.\n",
    "* `resume_from_checkpoint`: Start the training from a checkpoint saved earlier. Argument is the path to the saved model file.\n",
    "* `early_stop_callback`: Determine a metric to monitor for early stopping. The argument uses the callback class `EarlyStopping` that interrupts the training if the metric didn't improve for patience epochs. It can be applied as in the following code example \n",
    "\n",
    "```python\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(early_stop_callback=early_stopping)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "1. Source code with a nice introduction https://github.com/PyTorchLightning/pytorch-lightning\n",
    "2. Documentation, have a look at it, it's very well explained https://pytorch-lightning.readthedocs.io/en/stable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
