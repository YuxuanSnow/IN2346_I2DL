{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Introduction\n",
    "\n",
    "This is an introduction of PyTorch. It’s a Python-based scientific computing package targeted at two sets of audiences:\n",
    "\n",
    "- A replacement for NumPy to use the power of GPUs;\n",
    "\n",
    "- a deep learning research platform that provides maximum flexibility and speed.\n",
    "    - [`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html) is the central class of PyTorch.\n",
    "\n",
    "    - Central to all neural networks in PyTorch is the [`autograd`](https://pytorch.org/docs/stable/autograd.html)\n",
    "    package. It provides automatic differentiation for all\n",
    "    operations on Tensors. If we set the attribute `.requires_grad` of `torch.Tensor` as `True`, it starts to\n",
    "    track all operations on it. When finishing computation, we can call `.backward()` and have all the gradients\n",
    "    computed automatically. The gradient for this tensor will be accumulated into `.grad` attribute.\n",
    "\n",
    "\n",
    "## Goals of this tutorial\n",
    "\n",
    "- Understanding PyTorch's Tensor library and neural networks at a high level;\n",
    "\n",
    "- Training a small network with PyTorch;\n",
    "\n",
    "\n",
    "## Preparation\n",
    "\n",
    "- Install [PyTorch](https://pytorch.org/) and [torchvision](https://github.com/pytorch/vision) (CPU version); (**If you want to install a cuda version, remember to change the type of the following cell into markdown**)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install torch==1.4.0+cpu torchvision==0.5.0+cpu -f https://download.pytorch.org/whl/torch_stable.html > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- <div class=\"alert alert-block alert-info\"><b>(Optional)</b> You can also install a\n",
    "<a href=\"https://developer.nvidia.com/cuda-downloads\">Cuda</a>\n",
    "version if an Nvidia GPU and Cuda setup is installed on your machine, e.g.</div>\n",
    "\n",
    "```python\n",
    "# CUDA 10.1\n",
    "pip install torch==1.4.0+cu101 torchvision==0.5.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "```\n",
    "- <div class=\"alert alert-block alert-danger\">Make sure you've installed the <b>same version of PyTorch and\n",
    " torchvision</b>. If you install your own version, there might be some issues.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "print(f\"Torch version: {torch.__version__}\\nTorchvision version: {torchvision.__version__}\\n\")\n",
    "if not torch.__version__.startswith(\"1.4.0\"):\n",
    "    print(\"you are using an another version of PyTorch. We expect PyTorch 1.4.0. You can continue with your version but it\"\n",
    "          \" might cause some issues\")\n",
    "if not torchvision.__version__.startswith(\"0.5.0\"):\n",
    "    print(\"you are using an another version of torchvision. We expect torchvision 0.5.0. You can continue with your version but it\"\n",
    "          \" might cause some issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Getting Started\n",
    "\n",
    "In this session you will learn the basic element Tensor and some simple oprations of PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.1 Tensors\n",
    "\n",
    "Tensors are similar to NumPy’s ndarrays, with the addition being that Tensors can also be used on a GPU to accelerate\n",
    "computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Construct a (2,3) NumPy array and a (2,3) tensor directly from data\n",
    "# [[1 2 3]\n",
    "#  [4 5 6]]\n",
    "a_np = np.array([[1,2,3],[5,6,7]]) #NumPy array\n",
    "a_ts = torch.tensor([[1,2,3],[4,5,6]]) # Tensor\n",
    "print(\"a_np:\\n {},\\n Shape: {}\".format(type(a_np), a_np.shape))\n",
    "print(a_np)\n",
    "print(\"a_ts:\\n {},\\n Shape: {}\".format(type(a_ts), a_ts.shape)  )\n",
    "print(a_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.2 Conversion btw. NumPy ndarray and Tensor\n",
    "\n",
    "The conversion between NumPy ndarray and PyTorh tensor is quite easy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Conversion\n",
    "m_np = np.array([1, 2, 3])\n",
    "n_ts = torch.from_numpy(m_np) #Convert a numpy array to a Tensor\n",
    "\n",
    "v_np = n_ts.numpy() #Tensor to numpy\n",
    "v_np[1] = -1 #Numpy and Tensor share the same memory\n",
    "assert(m_np[1] == v_np[1]) #Change Numpy will also change the Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Hint:</b> During the conversion, both ndarray and Tensor share the same memory storage. Change value from either side will\n",
    "affect the other.</div>\n",
    "\n",
    "### 1.3 Operations\n",
    "\n",
    "#### 1.3.1 Indexing\n",
    "\n",
    "We can use the NumPy indexing in Tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Let us take the first two columns from the original array and save it in a new one\n",
    "b = a_ts[:2, :2] #Use numpy type indexing\n",
    "#b.shape\n",
    "b[:, 0] = 0 #For assignment\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Select elements which satisfy a condition\n",
    "# Using numpy array makes such a selection trivial\n",
    "mask = a_ts > 1\n",
    "new_array = a_ts[mask]\n",
    "print(new_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Do the same thing in a single step\n",
    "c = a_ts[a_ts>1]\n",
    "print(c == new_array) #Why assert doesn't work here\n",
    "##assert np.all(new_array == c) #  np.all() to indicate that all the values need to match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1.3.2 Mathematical operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Mathematical operations\n",
    "x = torch.tensor([[1,2],[3,4]])\n",
    "y = torch.tensor([[5,6],[7,8]])\n",
    "\n",
    "# Elementwise Addition\n",
    "# [[ 6.0  8.0]\n",
    "#  [10.0 12.0]]\n",
    "#Addition: syntax 1\n",
    "print(\"x + y: {}\".format(x + y))\n",
    "#Addition: syntax 2\n",
    "print(\"x + y: {}\".format(torch.add(x, y)))\n",
    "#Addition: syntax 3\n",
    "result_add = torch.empty(2, 2)\n",
    "torch.add(x, y, out=result_add)\n",
    "print(\"x + y: {}\".format(result_add))\n",
    "\n",
    "# Elementwise Subtraction\n",
    "# [[-4.0 -4.0]\n",
    "#  [-4.0 -4.0]]\n",
    "# Subtraction: syntax 1\n",
    "print(\"x - y: {}\".format(x - y))\n",
    "# Subtraction: syntax 2\n",
    "print(\"x - y: {}\".format(torch.sub(x, y)))\n",
    "# Subtraction: syntax 3\n",
    "result_sub = torch.empty(2, 2)\n",
    "torch.sub(x, y, out=result_sub)\n",
    "print(\"x - y: {}\".format(result_sub))\n",
    "\n",
    "# Elementwise Multiplication\n",
    "# [[ 5.0 12.0]\n",
    "#  [21.0 32.0]]\n",
    "# Multiplication: syntax 1\n",
    "print(\"x * y: {}\".format(x * y))\n",
    "# Multiplication: syntax 2\n",
    "print(\"x * y: {}\".format(torch.mul(x, y)))\n",
    "# Multiplication: syntax 3\n",
    "result_mul = torch.empty(2, 2)\n",
    "torch.mul(x, y, out=result_mul)\n",
    "print(\"x * y: {}\".format(result_mul))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "When dividing two ints in NumPy, the result is always a **float**, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_np = np.array([[1,2],[3,4]])\n",
    "y_np = np.array([[5,6],[7,8]])\n",
    "print(x_np / y_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "\n",
    "**However, in PyTorch 1.4.0 `torch.div` calculates floor division if both operands have integer types**;\n",
    "  If you want **true division** for integers, pleases convert the integers into floats first or specify the output as\n",
    "  `torch.div(a, b, out=c)`.\n",
    "<div class=\"alert alert-block alert-danger\">In PyTorch 1.5.0 you can use <b>true_divide</b> or <b>floor_divide</b>\n",
    " to calculate true division or floor division. And in future release div will perform true division as in Python 3. </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Elementwise Division\n",
    "# Floor Division: syntax 1\n",
    "print(\"x // y: {}\".format(x / y))\n",
    "# Floor Division: syntax 2\n",
    "print(\"x // y: {}\".format(torch.div(x, y)))\n",
    "# True Division: syntax 1\n",
    "result_true_div = torch.empty(2, 2)\n",
    "torch.div(x, y, out=result_true_div)\n",
    "print(\"x / y: {}\".format(result_true_div))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Devices\n",
    "\n",
    "When training a neural network, make sure that all the tensors are on the same device. Tensors can be moved onto any device using `.to` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "print(f\"Original device: {x.device}\") # \"cpu\", integer\n",
    "\n",
    "tensor = x.to(device)\n",
    "print(f\"Current device: {x.device}\") #\"cpu\" or \"cuda\", double"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So `x` has been moved onto cuda for those who have a GPU; otherwise it's still on the CPU.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Tip:</b> Include the <b>.to(device)</b> calls for every project such that\n",
    "you can easily port it to a GPU version.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Training a classifier with PyTorch\n",
    "\n",
    "In this session, you'll have an overview about how we could use PyTorch to load data, define neural networks, compute\n",
    "loss and make updates to the weights of the network.\n",
    "\n",
    "\n",
    "We will do the following steps in order:\n",
    "\n",
    "a) Dataloading in Pytorch compared to our previous datasets\n",
    "\n",
    "b) Define a two-layer network\n",
    "\n",
    "c) Define a loss function and optimizer\n",
    "\n",
    "d) Train the network\n",
    "\n",
    "e) Test the network\n",
    "\n",
    "### 2.1 Datasets and Loading\n",
    "\n",
    "The general procedure of dataloading is:\n",
    "\n",
    "a) Extract: Get the data from the source\n",
    "\n",
    "b) Transform: Put our data into suitable form (e.g. tensor form)\n",
    "\n",
    "c) Load: Put our data into an object to make it easily accessible\n",
    "\n",
    "#### 2.1.1 House price\n",
    "\n",
    "We'll use our dataloader and the dataloader of PyTorch to load the house price dataset separately.\n",
    "\n",
    "First, let's initialize our csv dataset from exercise 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_code.data.csv_dataset import CSVDataset, get_exercise5_transform\n",
    "from exercise_code.data.dataloader import DataLoader as our_DataLoader\n",
    "\n",
    "# dataloading and preprocessing steps as in ex04 2_logistic_regression.ipynb\n",
    "target_column = 'SalePrice'\n",
    "i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "root_path = os.path.join(i2dl_exercises_path, \"datasets\", 'housing')\n",
    "housing_file_path = os.path.join(root_path, \"housing_train.csv\")\n",
    "download_url = 'https://cdn3.vision.in.tum.de/~dl4cv/housing_train.zip'\n",
    "\n",
    "# Set up the transform to get two prepared columns\n",
    "select_two_columns_transform = get_exercise5_transform()\n",
    "\n",
    "# Set up the dataset\n",
    "our_csv_dataset = CSVDataset(target_column=target_column, root=root_path, download_url=download_url, mode=\"train\",\n",
    "                             transform=select_two_columns_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can set up our dataloader similar to Exercise 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our old dataloader\n",
    "batch_size = 4\n",
    "our_dataloader = our_DataLoader(our_csv_dataset, batch_size=batch_size)\n",
    "\n",
    "for i, item in enumerate(our_dataloader):\n",
    "    print('Starting item {}'.format(i))\n",
    "    print('item contains')\n",
    "    for key in item:\n",
    "        print(key)\n",
    "        print(type(item[key]))\n",
    "        print(item[key].shape)\n",
    "    \n",
    "    if i+1 >= 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In pyTorch we can directly use a [`Dataloader` class](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)\n",
    "and simply initalize it. And it also provides more parameters than ours, such as easy multiprocessing using `num_workers`. You can refer to the link\n",
    "to learn those additional supports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "pytorch_dataloader = DataLoader(our_csv_dataset, batch_size=batch_size)\n",
    "\n",
    "# We can use the exact same way to iterate over samples\n",
    "for i, item in enumerate(pytorch_dataloader):\n",
    "    print('Starting item {}'.format(i))\n",
    "    print('item contains')\n",
    "    for key in item:\n",
    "        print(key)\n",
    "        print(type(item[key]))\n",
    "        print(item[key].shape)\n",
    "    \n",
    "    if i+1 >= 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">As you can see, both dataloaders load the data with batch_size 4 and the data contains 2 features and 1 target. The only <b>difference</b> here is that the Dataloader of PyTorch will automatically transform the dataset into tensor format.</div>\n",
    "\n",
    "#### 2.1.2 Torchvision\n",
    "\n",
    "Specifically for vision, there's a package called `torchvision`, that has data loaders for common datasets such\n",
    "as Imagenet, FashionMNIST, MNIST, etc. and data transformers for images:\n",
    "`torchvision.datasets` and `torch.utils.data.DataLoader`.\n",
    "\n",
    "This provides a huge convenience and avoids writing boilerplate code.\n",
    "\n",
    "For this tutorial, we will use FashionMNIST dataset. It has 10 classes: 'T-shirt/top', 'Trouser', 'Pullover',\n",
    "'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'. The images in FashionMNIST\n",
    "are of size $1 \\times 28 \\times 28 $, i.e. 1-channel color images of $ 28 \\times 28 $ pixels in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a transform to convert images to tensor\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,),(0.5,))])  # mean and std have to be sequences (e.g. tuples),\n",
    "                                                                      # therefore we should add a comma after the values\n",
    "\n",
    "fashion_mnist_dataset = torchvision.datasets.FashionMNIST(root='../datasets', train=True,\n",
    "                                                          download=True, transform=transform)\n",
    "fashion_mnist_test_dataset = torchvision.datasets.FashionMNIST(root='../datasets', train=False,\n",
    "                                                          download=True, transform=transform)\n",
    "\n",
    "fashion_mnist_dataloader = DataLoader(fashion_mnist_dataset, batch_size=8)\n",
    "fashion_mnist_test_dataloader = DataLoader(fashion_mnist_test_dataset, batch_size=8)\n",
    "\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- `transforms.Compose` creates a series of transformation to prepare the dataset.\n",
    "\n",
    "- `transforms.ToTenser` convert `PIL image` or numpy.ndarray $(H \\times W\\times C)$ in the range [0,255] to a\n",
    "`torch.FloatTensor` of shape $(C \\times H \\times W)$ in the range [0.0, 1.0].\n",
    "\n",
    "- `transforms.Normalize` normalize a tensor image with mean and standard deviation.\n",
    "\n",
    "- `datasets.FashionMNIST` to download the Fashion MNIST datasets and transform the data.\n",
    "`train=True` if we want to get the training set; otherwise set `train=False` to get the\n",
    "test set.\n",
    "\n",
    "- `torch.utils.data.Dataloader` takes our training data or test data with parameter\n",
    "`batch_size` and `shuffle`. `batch_size` defines how many samples per batch to load.\n",
    "`shuffle=True` makes the data reshuffled at every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We can use the exact same way to iterate over samples\n",
    "for i, item in enumerate(fashion_mnist_dataloader):\n",
    "    print('Starting item {}'.format(i))\n",
    "    print('item contains')\n",
    "    image, label = item\n",
    "    print(f\"Type of input: {type(image)}\")\n",
    "    print(f\"Shape of the input: {image.shape}\")\n",
    "    print(f\"label: {label}\")\n",
    "\n",
    "    if i+1 >= 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Since we loaded the data with `batch_size` 8, the shape of the input is (8, 1, 28, 28). So before we push it into the affine layer, we need to flatten it with `x = x.view(-1, x.size[0)` (It will be shown later in 2.2)\n",
    "\n",
    "\n",
    "Let's show some of the training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5 # unormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(fashion_mnist_dataloader)\n",
    "images, labels = dataiter.next()\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.2 Define a Two-Layer Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "In exercise_06 we've defined the forward and backward pass for an affine layer and a Sigmoid layer\n",
    "(`exercise_code/networks/layer.py`) and completed the implementation of the `ClassificationionNet` class\n",
    "(`exercise_code/networks/classifiation_net.py`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_code.networks.classification_net import ClassificationNet\n",
    "hidden_size = 100\n",
    "std = 1.0\n",
    "model_ex06 = ClassificationNet(input_size=2, hidden_size=hidden_size, std=std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Have a look at your lengthy implementation first ;). Now, we can use `torch.nn.Module` to define our network class, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, activation=nn.Sigmoid(),\n",
    "                 input_size=1*28*28, hidden_size=100, classes=10):\n",
    "        super(Net, self).__init__()\n",
    "        self.input_size = input_size\n",
    "\n",
    "        # Here we initialize our activation and set up our two linear layers\n",
    "        self.activation = activation\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size) # flatten\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Similar to the `ClassificationNet` in exercise_06, here we defined a network with PyTorch.\n",
    "\n",
    " - PyTorch provides a `nn.Module` that builds neural networks\n",
    "\n",
    " - `super().__init__` creates a class that inherits attributes and behaviors from another\n",
    " class\n",
    "\n",
    " - `self.fc1` creates an affine layer with `input_size` inputs and `hidden_size` outputs.\n",
    "\n",
    " - `self.fc2` is similar to `self.fc1`.\n",
    "\n",
    " - `Forward` pass:\n",
    "\n",
    "    - first flatten the `x` with `x = x.view(-1, self.input_size)`\n",
    "\n",
    "    - 'Sandwich layer' by applying `fc1`, `activation`, `fc2` sequentially.\n",
    "    \n",
    "<div class=\"alert alert-block alert-info\">Thanks to <b>autograd</b> package, we just have to define the <b>forward</b> function. \n",
    " And the <b>backward</b> function (where gradients are computed) is automatically defined. We can use any of the Tensor operations in the <b>forward</b>  function.</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> We can use <b>print</b> to see all difined layers (but it won't show\n",
    "the information of the forward pass).\n",
    "\n",
    "And all the learnable parameters of a model are returned by <b>[model_name].parameters()</b>. We also have access to\n",
    "the parameters of different layers by <b>[model_name].[layer_name].parameters()</b> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#  create model\n",
    "net = Net()\n",
    "net = net.to(device) #always remember to move the network to the device\n",
    "\n",
    "print(net)\n",
    "\n",
    "for parameter in net.parameters():\n",
    "    print(parameter.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.3 Define a Loss function and optimizer\n",
    "\n",
    "Let's use a Classification Cross-Entropy loss and SGD with momentum.\n",
    "\n",
    "Recall that we've implemented SGD and MSE in exercise_04. Have a look at their implementations in\n",
    " `exercise_code/networks/optimizer.py` and `exercise_code/networks/loss.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_code.networks.optimizer import SGD\n",
    "from exercise_code.networks.loss import MSE, L1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we can import the loss function and optimizer directly from `torch.nn` and `torch.optim` respectively, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.4 Train the network\n",
    "\n",
    "This is when things start to get interesting. We simply have to loop over our data iterator, and feed the inputs to\n",
    "the network and optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_loss_history = [] # loss\n",
    "train_acc_history = [] # accuracy\n",
    "for epoch in range(2):\n",
    "\n",
    "    # TRAINING\n",
    "    running_loss = 0.0\n",
    "    correct = 0.0\n",
    "    total = 0\n",
    "    for i, data in enumerate(fashion_mnist_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        X, y = data\n",
    "\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        y_pred = net(X) # input x and predict based on x\n",
    "        loss = criterion(y_pred, y) # calculate the loss\n",
    "        loss.backward()  # backpropagation, compute gradients\n",
    "        optimizer.step()  # apply gradients\n",
    "\n",
    "        # loss and acc\n",
    "        running_loss += loss.item()\n",
    "        _, preds = torch.max(y_pred, 1) #convert output probabilities to predicted class\n",
    "        correct += preds.eq(y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "        # print statistics\n",
    "        if i % 1000 == 999: # print every 1000 mini-batches\n",
    "            running_loss /= 1000\n",
    "            correct /= total\n",
    "            print(\"[Epoch %d, Iteration %5d] loss: %.3f acc: %.2f %%\" % (epoch+1, i+1, running_loss, 100*correct))\n",
    "            train_loss_history.append(running_loss)\n",
    "            train_acc_history.append(correct)\n",
    "            running_loss = 0.0\n",
    "            correct = 0.0\n",
    "            total = 0\n",
    "\n",
    "print('FINISH.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "So the general training pass is as fowllows:\n",
    "\n",
    "- `zero_grad()`: zero the gradient buffers of all parameters and backprops with random gradient\n",
    "\n",
    "- `y_pred = net(X)`: make a forward pass through the network to getting log probabilities by passing the\n",
    "images to the model.\n",
    "\n",
    "- `loss = criterion(y_pred, y)`: calculate the loss\n",
    "\n",
    "- `loss.backward()`: perform a backward pass through the network to calculate the gradients for model parameters.\n",
    "\n",
    "-  `optimizer.step()`: take a step with the optimizer to update the model parameters.\n",
    "\n",
    "We keep tracking the training loss and accuracy over time. The following plot shows averages values for train loss and\n",
    "accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(train_acc_history)\n",
    "plt.plot(train_loss_history)\n",
    "plt.title(\"FashionMNIST\")\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('acc/loss')\n",
    "plt.legend(['acc', 'loss'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Test the network on the test data\n",
    "\n",
    "We have trained the network for 2 passes over the training dataset. Now we want to check\n",
    "the model by predicting the class label that the neural network outputs, and checking it\n",
    "against the ground-truth. If the prediction is correct, we add the sample to the list of\n",
    "correct predictions.\n",
    "\n",
    "And we'll visualize the data to display test images and their labels in the following format: `predicted (ground-truth)`. The text will be green for accurately classified examples and red for incorrect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#obtain one batch of test images\n",
    "dataiter = iter(fashion_mnist_test_dataloader)\n",
    "images, labels = dataiter.__next__()\n",
    "\n",
    "# get sample outputs\n",
    "outputs = net(images)\n",
    "# convert output probabilites to predicted class\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# prep images for display\n",
    "images = images.numpy()\n",
    "\n",
    "# plot the images in the batch, along with predicted and true labels\n",
    "fig = plt.figure(figsize=(25,4))\n",
    "for idx in range(8):\n",
    "    ax = fig.add_subplot(2, 8/2, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    ax.set_title(f\"{classes[predicted[idx]]} ({classes[labels[idx]]})\",\n",
    "                color=\"green\" if predicted[idx]==labels[idx] else \"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can also show what are the classes that performed well, and the classes that did not perform well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in fashion_mnist_test_dataloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %11s: %2d %%' % (classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Reference\n",
    "\n",
    "1. [PyTorch Tutorial](https://pytorch.org/tutorials/)\n",
    "\n",
    "2. [Fashion MNIST dataset training using PyTorch](https://medium.com/@aaysbt/fashion-mnist-data-training-using-pytorch-7f6ad71e96f4)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
