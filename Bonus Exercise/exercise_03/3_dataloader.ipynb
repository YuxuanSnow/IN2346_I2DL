{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader\n",
    "In the previous two notebooks you have implemented two different datasets that we can now use to access our data. However, in machine learning, we often need to perform a few additional data preparation steps before we can start training models.\n",
    "\n",
    "An important additional class for data preparation is the **DataLoader**. By wrapping a dataset in a dataloader, we will be able to load small subsets of the dataset at a time, instead of having to load each sample separately. In machine learning, the small subsets are referred to as **mini-batches**, which will play an important role later in the lecture.\n",
    "\n",
    "In this notebook, you will implement your own dataloader, which you can then use to load mini-batches from the datasets you implemented previously.\n",
    "\n",
    "First, we need to import libraries and code, as always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from exercise_code.data import DataLoader, DummyDataset\n",
    "from exercise_code.networks import DummyNetwork\n",
    "from exercise_code.tests import test_dataloader, save_pickle\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating over a Dataset\n",
    "Throughout this notebook we will use a dummy dataset that contains all even numbers from 2 to 100. Similar to the datasets you have implemented before, the dummy dataset has a `__len__()` method that allows us to call `len(dataset)`, as well as a `__getitem__()` method, which allows us to call `dataset[i]` and returns a dict `{\"data\": val}` where `val` is the i-th even number. If you would like to see the code, have a look at `DummyDataset` in `exercise_code/data/base_dataset.py`.\n",
    "\n",
    "Let's start by defining the dataset, and calling it's methods to get a better feel for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Length:\t 50 \n",
      "First Element:\t {'data': 2} \n",
      "Last Element:\t {'data': 100}\n"
     ]
    }
   ],
   "source": [
    "dataset = DummyDataset(\n",
    "    root=None,\n",
    "    divisor=2,\n",
    "    limit=100\n",
    ")\n",
    "print(\n",
    "    \"Dataset Length:\\t\", len(dataset),\n",
    "    \"\\nFirst Element:\\t\", dataset[0],\n",
    "    \"\\nLast Element:\\t\", dataset[-1],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we will write some code to iterate over the dataset in mini-batches, similar to what we would like our dataloader to do. The number of samples to load per mini-batch is referred to as **batch size**. For the remainder of this notebook, let's use a batch size of 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now define a simple function that iterates over the dataset and groups samples into mini-batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_batches(dataset, batch_size):\n",
    "    batches = []  # list of all mini-batches\n",
    "    batch = []  # curernt mini-batch\n",
    "    for i in range(len(dataset)):\n",
    "        batch.append(dataset[i]) #add the element of dataset into the mini-batch\n",
    "        if len(batch) == batch_size:  # if the current mini-batch is full,\n",
    "            batches.append(batch)  # add it to the list of mini-batches,\n",
    "            #print(batch)\n",
    "            batch = []  # and start a new mini-batch\n",
    "    #print(batches)\n",
    "    return batches\n",
    "\n",
    "batches = build_batches(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's have a look at our mini-batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mini-batch 0: [{'data': 2}, {'data': 4}, {'data': 6}]\n",
      "mini-batch 1: [{'data': 8}, {'data': 10}, {'data': 12}]\n",
      "mini-batch 2: [{'data': 14}, {'data': 16}, {'data': 18}]\n",
      "mini-batch 3: [{'data': 20}, {'data': 22}, {'data': 24}]\n",
      "mini-batch 4: [{'data': 26}, {'data': 28}, {'data': 30}]\n",
      "mini-batch 5: [{'data': 32}, {'data': 34}, {'data': 36}]\n",
      "mini-batch 6: [{'data': 38}, {'data': 40}, {'data': 42}]\n",
      "mini-batch 7: [{'data': 44}, {'data': 46}, {'data': 48}]\n",
      "mini-batch 8: [{'data': 50}, {'data': 52}, {'data': 54}]\n",
      "mini-batch 9: [{'data': 56}, {'data': 58}, {'data': 60}]\n",
      "mini-batch 10: [{'data': 62}, {'data': 64}, {'data': 66}]\n",
      "mini-batch 11: [{'data': 68}, {'data': 70}, {'data': 72}]\n",
      "mini-batch 12: [{'data': 74}, {'data': 76}, {'data': 78}]\n",
      "mini-batch 13: [{'data': 80}, {'data': 82}, {'data': 84}]\n",
      "mini-batch 14: [{'data': 86}, {'data': 88}, {'data': 90}]\n",
      "mini-batch 15: [{'data': 92}, {'data': 94}, {'data': 96}]\n"
     ]
    }
   ],
   "source": [
    "def print_batches(batches):  \n",
    "    for i, batch in enumerate(batches):\n",
    "        print(\"mini-batch %d:\" % i, str(batch))\n",
    "\n",
    "print_batches(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the iteration works, but the output is not very pretty. Let us now write a simple function that combines the dictionaries of samples in a mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mini-batch 0: {'data': [2, 4, 6]}\n",
      "mini-batch 1: {'data': [8, 10, 12]}\n",
      "mini-batch 2: {'data': [14, 16, 18]}\n",
      "mini-batch 3: {'data': [20, 22, 24]}\n",
      "mini-batch 4: {'data': [26, 28, 30]}\n",
      "mini-batch 5: {'data': [32, 34, 36]}\n",
      "mini-batch 6: {'data': [38, 40, 42]}\n",
      "mini-batch 7: {'data': [44, 46, 48]}\n",
      "mini-batch 8: {'data': [50, 52, 54]}\n",
      "mini-batch 9: {'data': [56, 58, 60]}\n",
      "mini-batch 10: {'data': [62, 64, 66]}\n",
      "mini-batch 11: {'data': [68, 70, 72]}\n",
      "mini-batch 12: {'data': [74, 76, 78]}\n",
      "mini-batch 13: {'data': [80, 82, 84]}\n",
      "mini-batch 14: {'data': [86, 88, 90]}\n",
      "mini-batch 15: {'data': [92, 94, 96]}\n"
     ]
    }
   ],
   "source": [
    "def combine_batch_dicts(batch):\n",
    "    batch_dict = {}\n",
    "    for data_dict in batch:\n",
    "        for key, value in data_dict.items():\n",
    "            if key not in batch_dict:\n",
    "                batch_dict[key] = []\n",
    "            batch_dict[key].append(value)\n",
    "    return batch_dict\n",
    "\n",
    "combined_batches = [combine_batch_dicts(batch) for batch in batches]\n",
    "print_batches(combined_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much more organized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform operations more efficiently later, we would also like the values of the mini-batches to be contained in a numpy array instead of a simple list. Let's briefly write a function for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mini-batch 0: {'data': array([2, 4, 6])}\n",
      "mini-batch 1: {'data': array([ 8, 10, 12])}\n",
      "mini-batch 2: {'data': array([14, 16, 18])}\n",
      "mini-batch 3: {'data': array([20, 22, 24])}\n",
      "mini-batch 4: {'data': array([26, 28, 30])}\n",
      "mini-batch 5: {'data': array([32, 34, 36])}\n",
      "mini-batch 6: {'data': array([38, 40, 42])}\n",
      "mini-batch 7: {'data': array([44, 46, 48])}\n",
      "mini-batch 8: {'data': array([50, 52, 54])}\n",
      "mini-batch 9: {'data': array([56, 58, 60])}\n",
      "mini-batch 10: {'data': array([62, 64, 66])}\n",
      "mini-batch 11: {'data': array([68, 70, 72])}\n",
      "mini-batch 12: {'data': array([74, 76, 78])}\n",
      "mini-batch 13: {'data': array([80, 82, 84])}\n",
      "mini-batch 14: {'data': array([86, 88, 90])}\n",
      "mini-batch 15: {'data': array([92, 94, 96])}\n"
     ]
    }
   ],
   "source": [
    "def batch_to_numpy(batch):\n",
    "    numpy_batch = {}\n",
    "    for key, value in batch.items():\n",
    "        numpy_batch[key] = np.array(value)\n",
    "    return numpy_batch\n",
    "\n",
    "numpy_batches = [batch_to_numpy(batch) for batch in combined_batches]\n",
    "print_batches(numpy_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we would like to make the loading a bit more memory efficient. Instead of loading the entire dataset into memory at once, let us only load samples when they are needed. We can do so by building a Python generator, using the `yield` keyword. See https://wiki.python.org/moin/Generators for more information on generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mini-batch 0: {'data': array([ 4, 80, 68])}\n",
      "mini-batch 1: {'data': array([100,   2,  58])}\n",
      "mini-batch 2: {'data': array([82, 44, 30])}\n",
      "mini-batch 3: {'data': array([64, 22, 10])}\n",
      "mini-batch 4: {'data': array([54, 12, 26])}\n",
      "mini-batch 5: {'data': array([94, 72, 16])}\n",
      "mini-batch 6: {'data': array([74, 76, 18])}\n",
      "mini-batch 7: {'data': array([84, 14, 38])}\n",
      "mini-batch 8: {'data': array([ 6,  8, 28])}\n",
      "mini-batch 9: {'data': array([88, 42, 70])}\n",
      "mini-batch 10: {'data': array([66, 78, 48])}\n",
      "mini-batch 11: {'data': array([40, 62, 32])}\n",
      "mini-batch 12: {'data': array([52, 36, 46])}\n",
      "mini-batch 13: {'data': array([50, 86, 34])}\n",
      "mini-batch 14: {'data': array([60, 24, 92])}\n",
      "mini-batch 15: {'data': array([20, 56, 96])}\n"
     ]
    }
   ],
   "source": [
    "def build_batch_iterator(dataset, batch_size, shuffle):\n",
    "    if shuffle:\n",
    "        index_iterator = iter(np.random.permutation(len(dataset)))  # define indices as iterator\n",
    "    else:\n",
    "        index_iterator = iter(range(len(dataset)))  # define indices as iterator\n",
    "\n",
    "    batch = []\n",
    "    for index in index_iterator:  # iterate over indices using the iterator\n",
    "        batch.append(dataset[index])\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch  # use yield keyword to define a iterable generator\n",
    "            batch = []\n",
    "            \n",
    "batch_iterator = build_batch_iterator(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "batches = []\n",
    "for batch in batch_iterator:\n",
    "    batches.append(batch)\n",
    "\n",
    "print_batches(\n",
    "    [batch_to_numpy(combine_batch_dicts(batch)) for batch in batches]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functionality of the cell above is now pretty close to what we would like our dataloader to do. However, there are still two remaining issues:\n",
    "1. The last two samples of our dataset are not contained in any mini-batches. This is because the number of samples in our dataset is not dividable by the batch size, so there are a few left-over samples which are implicitly discarded. Ideally, we would like to have an option that allows us to decide how to handle these last samples.\n",
    "2. The order of mini-batches, as well as which samples are grouped together, is always in increasing order. Ideally, we would also like to have an option that allows us to randomize which samples are grouped together. The randomization could be easily implemented by randomly permuting the indices of the dataset before iterating over it, e.g. using `indices = np.random.permutation(len(dataset))`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: DataLoader Class Implementation\n",
    "Now it is your turn to put everything together and implement the DataLoader as a proper class.\n",
    "We provide you with a basic skeleton for this, which you can find in  `exercise_code/data/dataloader.py`. Open the file and have a look at the class. Note that the `__init__` method receives four arguments:\n",
    "* **dataset** is the dataset that the dataloader should load.\n",
    "* **batch_size** is the mini-batch size, i.e. the number of samples you want to load at the same time.\n",
    "* **shuffle** is binary and defines whether the dataset should be randomly shuffled or not.\n",
    "* **drop_last**: is binary and defines how to handle the last mini-batch in your dataset. Specifically, if the amount of samples in your dataset is not dividable by the minibatch size then there will be some samples left over in the end. If `drop_last=True`, we simply discard those samples, otherwise we return them together as a smaller minibatch.\n",
    "\n",
    "\n",
    "Your task is now to implement the two core methods of the dataloader:\n",
    "* `__len__(self)` should return the length of the dataloader, which corresponds to the number of minibatches that you can load from the dataset. Similar to your datasets, this will allows you to call `len(dataloader)` later.\n",
    "* `__iter__(self)` should define how to iterate over the dataloader. For this, you should define an iterable again, similar to what we have done in the `build_batch_iterator()` function above. The mini-batches loaded by `__iter__()` should each be a dict consisting of numpy arrays.\n",
    "\n",
    "If you're done, run the cells below to check if your dataloader works as intended.\n",
    "\n",
    "**Hint:** Make use of the code above when implementing your `__iter__()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': [72, 16, 6]}\n",
      "{'data': [68, 86, 20]}\n",
      "{'data': [30, 34, 66]}\n",
      "{'data': [96, 10, 2]}\n",
      "{'data': [24, 78, 46]}\n",
      "{'data': [28, 92, 64]}\n",
      "{'data': [76, 40, 100]}\n",
      "{'data': [4, 54, 22]}\n",
      "{'data': [88, 50, 62]}\n",
      "{'data': [8, 74, 90]}\n",
      "{'data': [36, 48, 80]}\n",
      "{'data': [32, 44, 94]}\n",
      "{'data': [60, 56, 12]}\n",
      "{'data': [38, 18, 98]}\n",
      "{'data': [14, 84, 70]}\n",
      "{'data': [58, 42, 26]}\n",
      "{'data': [82, 52]}\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LenTestInt passed.\n",
      "LenTestCorrect passed.\n",
      "Method __len__() correctly implemented. Tests passed: 2/2\n",
      "IterTestIterable passed.\n",
      "IterTestItemType passed.\n",
      "IterTestBatchSize passed.\n",
      "IterTestNumBatches passed.\n",
      "IterTestValuesUnique passed.\n",
      "IterTestValueRange passed.\n",
      "IterTestShuffled passed.\n",
      "IterTestNonDeterministic passed.\n",
      "Method __iter__() correctly implemented. Tests passed: 8/8\n",
      "Class DataLoader correctly implemented. Tests passed: 10/10\n",
      "Score: 100/100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader1 = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "test_dataloader(\n",
    "    dataset=dataset,\n",
    "    dataloader=dataloader1,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LenTestInt passed.\n",
      "LenTestCorrect passed.\n",
      "Method __len__() correctly implemented. Tests passed: 2/2\n",
      "IterTestIterable passed.\n",
      "IterTestItemType passed.\n",
      "IterTestBatchSize passed.\n",
      "IterTestNumBatches passed.\n",
      "IterTestValuesUnique passed.\n",
      "IterTestValueRange passed.\n",
      "IterTestShuffled passed.\n",
      "IterTestNonDeterministic passed.\n",
      "Method __iter__() correctly implemented. Tests passed: 8/8\n",
      "Class DataLoader correctly implemented. Tests passed: 10/10\n",
      "Score: 100/100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader2 = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False\n",
    ")\n",
    "test_dataloader(\n",
    "    dataset=dataset,\n",
    "    dataloader=dataloader2,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save your DataLoaders for Submission\n",
    "Simply save your dataloaders using the following cell. This will save them to a pickle file `models/dataloader.p`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(\n",
    "    data_dict={\n",
    "        \"dataloader1\": dataloader1,\n",
    "        \"dataloader2\": dataloader2,\n",
    "    },\n",
    "    file_name=\"dataloader.p\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "1. In machine learning, we often need to load data in **mini-batches**, which are small subsets of the training dataset. How many samples to load per mini-batch is called the **batch size**.\n",
    "2. In addition to the Dataset class, we use a **DataLoader** class that takes care of mini-batch construction, data shuffling, and more.\n",
    "3. The dataloader is iterable and only loads those samples of the dataset that are needed for the current mini-batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Instructions\n",
    "\n",
    "Now, that you have completed the neccessary parts in the notebook, you can go on and submit your files.\n",
    "\n",
    "1. Go on [our submission page](https://dvl.in.tum.de/teaching/submission/), register for an account and login. We use your matriculation number and send an email with the login details to the mail account associated. When in doubt, login into tum online and check your mails there. You will get an id which we need in the next step.\n",
    "2. Navigate to `exercise_code` directory and run the `create_submission.sh` file to create the zip file of your model. This will create a single `zip` file that you need to upload. Otherwise, you can also zip it manually if you don't want to use the bash script.\n",
    "3. Log into [our submission page](https://dvl.in.tum.de/teaching/submission/) with your account details and upload the `zip` file. Once successfully uploaded, you should be able to see the submitted \"dummy_model.p\" file selectable on the top.\n",
    "4. Click on this file and run the submission script. You will get an email with your score as well as a message if you have surpassed the threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/i2dlsubmission.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Goals\n",
    "\n",
    "- Goal: Implement a DataLoader that loads mini-batches from a given dataset and supports batch_size, shuffle, and drop_last args.\n",
    "- Test cases:\n",
    "  1. Does `__len__()` return the correct data type?\n",
    "  2. Does `__len__()` return the correct value?\n",
    "  3. Does `__iter__()` work at all, i.e. is it possible to iterate over the dataloader?\n",
    "  4. Does `__iter__()` load the correct data type?\n",
    "  5. Does `__iter__()` load data with correct batch size?\n",
    "  6. Does `__iter__()` load the correct number of batches?\n",
    "  7. Does `__iter__()` load every sample only once?\n",
    "  8. Does `__iter__()` load the smallest and largest sample from the dataset?\n",
    "  9. Does `__iter__()` shuffle the data correctly (if necessary)?\n",
    "  10. Does `__iter__()` return non-deterministic values when shuffling?\n",
    "- Reachable points [0, 100]: 0 if not implemented, 100 if all tests passed, 10 per passed test\n",
    "- Threshold to clear exercise: 80\n",
    "- Submission start: __May 11, 2020 23.59__\n",
    "- Submission deadline : __May 17, 2020 23.59__. \n",
    "- You can make multiple submission uptil the deadline. Your __best submission__ will be considered for bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlook\n",
    "You have now implemented everything you need to use the CIFAR and House Prices datasets for deep learning model training. Using your dataset and dataloader, your model training will later look something like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary -: 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-720c74cc83a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDummyNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dummy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mminibatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;31m# do more stuff... (soon)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/I2DL/exercise_03/i2dl_exercises/exercise_03/exercise_code/networks/base_networks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m########################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m  \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: bad operand type for unary -: 'dict'"
     ]
    }
   ],
   "source": [
    "dataset = DummyDataset(\n",
    "    root=None,\n",
    "    divisor=2,\n",
    "    limit=200,\n",
    ")\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=3,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "model = DummyNetwork(model_name=\"dummy\")\n",
    "for minibatch in dataloader:\n",
    "    model_output = model.forward(minibatch)\n",
    "    # do more stuff... (soon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
